# -*- coding: utf-8 -*-
"""crisis-llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ylhva9x348mBO6UY5OH1zSIOJGyGRnj6
"""

!pip install ipywidgets==7.7.1

import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub.hf_api import HfFolder

ACCESS_TOKEN = "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"

HfFolder.save_token(ACCESS_TOKEN)

import torch
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import json
import os
import pandas as pd

# Check if GPU is available
device = 0 if torch.cuda.is_available() else -1

# Optional: Set environment variables for memory allocation optimization
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'

# Load the model and tokenizer for DistilGPT2 (smaller LLM)
model_name = "distilgpt2"  # A smaller LLM model, alternative to GPT-3.5
pipe = pipeline("text-generation", model=model_name, device=device)

# Alternatively, you can load the tokenizer and model explicitly if needed:
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Set the padding token to eos_token (commonly used as the padding token for models without a pad_token)
tokenizer.pad_token = tokenizer.eos_token

# Load incident data from the uploaded JSON file
file_path = '/kaggle/input/crisis-llm-json/CM_JSON.json'  # Update this path if needed
with open(file_path, 'r') as f:
    incident_data = json.load(f)

# If the data is a list, process the incidents
if isinstance(incident_data, list):
    incidents = incident_data
else:
    incidents = [incident_data]  # Wrap in a list if only a single incident is present

# Define the communication template with structured headings
template = """
Crisis Management Response:
============================
**Incident Summary**:
{incident_summary}

**Impact**:
{impact}

**Immediate Actions**:
{immediate_actions}

**Recommended Next Steps**:
{recommended_next_steps}

**Resources**:
{resources}
"""

# Function to generate a communication template based on the provided template, using the incident's data
def generate_communication_from_template(incident):
    """
    Generate a communication based on the provided template, using the incident's data.
    """
    # Populate the template using the provided incident data
    incident_summary = incident.get('description', 'No description available') + " " * 20  # Add extra content
    impact = f"Severity: {incident.get('severity', 'Unknown severity')}, Affected Assets: {', '.join(incident.get('affected_assets', []))} " * 5  # Expand impact section
    immediate_actions = "Isolate affected network components, start immediate diagnostics, involve technical teams, and begin root cause analysis. Ensure continuous communication with stakeholders."  # Expanded actions
    recommended_next_steps = "Conduct thorough investigation, communicate with affected users, coordinate with cybersecurity and network teams to apply mitigation strategies. Ensure all impacted systems are restored."  # Expanded steps
    resources = "Cybersecurity team, network engineers, customer support for communication, incident management team, and external partners for advanced mitigation support."  # Expanded resources

    # Fill the template with the incident data
    filled_template = template.format(
        incident_summary=incident_summary,
        impact=impact,
        immediate_actions=immediate_actions,
        recommended_next_steps=recommended_next_steps,
        resources=resources
    )

    # Explicitly truncate text if it exceeds the max_length and pad the text
    inputs = tokenizer(filled_template, return_tensors="pt", max_length=512, truncation=True, padding=True)

    # Create the attention mask
    attention_mask = inputs['attention_mask']

    # Generate the communication using the model with sampling enabled
    result = model.generate(
        inputs['input_ids'],
        attention_mask=attention_mask,  # Pass the attention mask
        max_length=512,
        num_return_sequences=1,
        temperature=0.7,  # Enable sampling
        do_sample=True  # Set do_sample=True to use the temperature parameter
    )

    # Decode the generated text
    generated_text = tokenizer.decode(result[0], skip_special_tokens=True)

    return generated_text

# Initialize an empty list to store the responses
responses = []

# Process each incident and generate the communication template
for incident in incidents:
    response = generate_communication_from_template(incident)
    responses.append({
        'incident_id': incident.get('incident_id', 'N/A'),
        'communication_template': response
    })

# Create a DataFrame to store all responses
df = pd.DataFrame(responses)

# Save the DataFrame to a CSV file
output_file = '/kaggle/working/crisis_management_responses.csv'
df.to_csv(output_file, index=False)

# Print the generated communication template for the first incident
print("Generated Crisis Communication Template:")
print(df['communication_template'].iloc[0])

# Display the DataFrame (for previewing)
df.head()
