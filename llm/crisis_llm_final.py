# -*- coding: utf-8 -*-
"""Crisis_LLM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/crisis-llm-4224cbbc-483c-4fd9-ab18-d536c101cd6d.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241122/auto/storage/goog4_request%26X-Goog-Date%3D20241122T035737Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D04614d6cf1d29364259b8f31751293acdba03f1dc7af98be8881405245d14442720917f5a0bd8a94f20473b642840d06e30f058e3bdb501c68a9a83f671ce1553bebd8b4d2b470f97f9ff647ce0ad66faf40a6b36291b947fd41b0792805a40a4690e6316d353f177dac16e458f5f32ef9f94252902d80dcf5140874df87900edf30b002d1c579afdb6956de6a2da1d92ce597143090bbd136473b401f67e8d0e5d5e36027c83d2cea92618fa1b0b41ffdfca7a68afd4bb764dc7a3fe8ecdb03fa23b7be6ade5f1f25d2133f20c297db8b9880ba67084828057e47f01cf6a9c307fe3bc862f4dda026f397d16f882be6265b99a0935ad26c1a95ae01317c905c
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

altheapotter_crisis_llm_json_path = kagglehub.dataset_download('altheapotter/crisis-llm-json')

print('Data source import complete.')

!pip install ipywidgets==7.7.1

import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub.hf_api import HfFolder

ACCESS_TOKEN = "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"

HfFolder.save_token(ACCESS_TOKEN)

import torch
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import json
import os
import pandas as pd

# Check if GPU is available
device = 0 if torch.cuda.is_available() else -1

# Optional: Set environment variables for memory allocation optimization
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'

# Load the model and tokenizer for DistilGPT2 (smaller LLM)
model_name = "distilgpt2"  # A smaller LLM model, alternative to GPT-3.5
pipe = pipeline("text-generation", model=model_name, device=device)

# Alternatively, you can load the tokenizer and model explicitly if needed:
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Set the padding token to eos_token (commonly used as the padding token for models without a pad_token)
tokenizer.pad_token = tokenizer.eos_token

# Load incident data from the uploaded JSON file
file_path = '/kaggle/input/crisis-llm-json/CM_JSON.json'  # Update this path if needed
with open(file_path, 'r') as f:
    incident_data = json.load(f)

# If the data is a list, process the incidents
if isinstance(incident_data, list):
    incidents = incident_data
else:
    incidents = [incident_data]  # Wrap in a list if only a single incident is present

# Define the communication template with structured headings
template = """
Crisis Management Response:
============================
**Incident Summary**:
{incident_summary}

**Impact**:
{impact}

**Immediate Actions**:
{immediate_actions}

**Recommended Next Steps**:
{recommended_next_steps}

**Resources**:
{resources}
"""

# Function to generate a communication template based on the provided template, using the incident's data
def generate_communication_from_template(incident):
    """
    Generate a communication based on the provided template, using the incident's data.
    """
    # Populate the template using the provided incident data
    incident_summary = incident.get('description', 'No description available') + " " * 20  # Add extra content
    impact = f"Severity: {incident.get('severity', 'Unknown severity')}, Affected Assets: {', '.join(incident.get('affected_assets', []))} " * 5  # Expand impact section
    immediate_actions = "Isolate affected network components, start immediate diagnostics, involve technical teams, and begin root cause analysis. Ensure continuous communication with stakeholders."  # Expanded actions
    recommended_next_steps = "Conduct thorough investigation, communicate with affected users, coordinate with cybersecurity and network teams to apply mitigation strategies. Ensure all impacted systems are restored."  # Expanded steps
    resources = "Cybersecurity team, network engineers, customer support for communication, incident management team, and external partners for advanced mitigation support."  # Expanded resources

    # Fill the template with the incident data
    filled_template = template.format(
        incident_summary=incident_summary,
        impact=impact,
        immediate_actions=immediate_actions,
        recommended_next_steps=recommended_next_steps,
        resources=resources
    )

    # Explicitly truncate text if it exceeds the max_length and pad the text
    inputs = tokenizer(filled_template, return_tensors="pt", max_length=512, truncation=True, padding=True)

    # Create the attention mask
    attention_mask = inputs['attention_mask']

    # Generate the communication using the model with sampling enabled
    result = model.generate(
        inputs['input_ids'],
        attention_mask=attention_mask,  # Pass the attention mask
        max_length=512,
        num_return_sequences=1,
        temperature=0.7,  # Enable sampling
        do_sample=True  # Set do_sample=True to use the temperature parameter
    )

    # Decode the generated text
    generated_text = tokenizer.decode(result[0], skip_special_tokens=True)

    return generated_text

# Initialize an empty list to store the responses
responses = []

# Process each incident and generate the communication template
for incident in incidents:
    response = generate_communication_from_template(incident)
    responses.append({
        'incident_id': incident.get('incident_id', 'N/A'),
        'communication_template': response
    })

# Create a DataFrame to store all responses
df = pd.DataFrame(responses)

# Save the DataFrame to a CSV file
output_file = '/kaggle/working/crisis_management_responses.csv'
df.to_csv(output_file, index=False)

# Print the generated communication template for the first incident
print("Generated Crisis Communication Template:")
print(df['communication_template'].iloc[0])

# Display the DataFrame (for previewing)
df.head()

# Add the generated response to each incident and update the JSON
for incident in incidents:
    response = generate_communication_from_template(incident)
    incident['crisis_response'] = response

# Save the updated incidents back to a JSON file
output_file = '/kaggle/working/updated_CM_JSON.json'
with open(output_file, 'w') as f:
    json.dump(incidents, f, indent=4)

# Print success message and the location of the output file
print(f"Updated JSON with crisis responses saved to: {output_file}")

import json
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

# Check if GPU is available
device = 0 if torch.cuda.is_available() else -1

# Load the model and tokenizer
model_name = "distilgpt2"
pipe = pipeline("text-generation", model=model_name, device=device)
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Set the padding token to eos_token
tokenizer.pad_token = tokenizer.eos_token

# Load the JSON file
input_file_path = '/kaggle/input/crisis-llm-json/CM_JSON.json'  # Update if needed
output_file_path = '/kaggle/working/Updated_CM_JSON.json'

with open(input_file_path, 'r') as f:
    incident_data = json.load(f)

# Ensure the data is a list
if isinstance(incident_data, dict):
    incidents = [incident_data]
else:
    incidents = incident_data

# Templates
crisis_template = """
Crisis Management Response:
============================
**Incident Summary**:
{incident_summary}

**Impact**:
{impact}

**Immediate Actions**:
{immediate_actions}

**Recommended Next Steps**:
{recommended_next_steps}

**Resources**:
{resources}
"""

post_incident_template = """
Post-Incident Analysis:
========================
The recent incident involves {incident_description}, affecting {affected_assets}. Immediate actions included {immediate_actions_taken}, which effectively {containment_summary}.

The recommended next steps focus on {mitigation_summary}, ensuring compliance and risk reduction. It is essential to engage with {engaged_resources} to {post_action_summary}.

**Classification**: This incident qualifies as a "{classification}". Future efforts should include {future_focus}.
========================
"""

# Functions for generating templates
def generate_communication_from_template(incident):
    incident_summary = incident.get('description', 'No description available')
    impact = f"Severity: {incident.get('severity', 'Unknown severity')}, Affected Assets: {', '.join(incident.get('affected_assets', []))}"
    immediate_actions = "Isolate affected network components, start immediate diagnostics, involve technical teams, and begin root cause analysis."
    recommended_next_steps = "Conduct thorough investigation, communicate with affected users, coordinate with cybersecurity and network teams to apply mitigation strategies."
    resources = "Cybersecurity team, network engineers, customer support for communication, incident management team."

    filled_template = crisis_template.format(
        incident_summary=incident_summary,
        impact=impact,
        immediate_actions=immediate_actions,
        recommended_next_steps=recommended_next_steps,
        resources=resources
    )

    inputs = tokenizer(filled_template, return_tensors="pt", max_length=512, truncation=True, padding=True)
    result = model.generate(
        inputs['input_ids'],
        max_length=512,
        num_return_sequences=1,
        temperature=0.7,
        do_sample=True
    )

    return tokenizer.decode(result[0], skip_special_tokens=True)

def generate_post_incident_analysis(incident, communication_template):
    incident_description = incident.get('description', 'an incident')
    affected_assets = ', '.join(incident.get('affected_assets', [])) or "key systems"
    immediate_actions_taken = "isolating affected systems and initiating investigative protocols"
    containment_summary = "contained the issue and limited its impact"
    mitigation_summary = "enhancing existing protocols and ensuring compliance with industry standards"
    engaged_resources = "legal and forensic resources"
    post_action_summary = "analyze and reinforce the organization's preparedness"
    classification = "Network Incident" if "network" in incident_description.lower() else "General Incident"
    future_focus = "regular system audits, incident drills, and improved employee training"

    filled_template = post_incident_template.format(
        incident_description=incident_description,
        affected_assets=affected_assets,
        immediate_actions_taken=immediate_actions_taken,
        containment_summary=containment_summary,
        mitigation_summary=mitigation_summary,
        engaged_resources=engaged_resources,
        post_action_summary=post_action_summary,
        classification=classification,
        future_focus=future_focus
    )

    return filled_template

# Process incidents and add new fields
for incident in incidents:
    communication_template = generate_communication_from_template(incident)
    post_incident_analysis = generate_post_incident_analysis(incident, communication_template)
    incident['communication_template'] = communication_template
    incident['post-incident-response'] = post_incident_analysis

# Save the updated incidents back to a JSON file
with open(output_file_path, 'w') as f:
    json.dump(incidents, f, indent=4)

# Print confirmation and example
print(f"Updated JSON saved to: {output_file_path}")
print("\nExample of an updated incident:")
print(json.dumps(incidents[0], indent=4))